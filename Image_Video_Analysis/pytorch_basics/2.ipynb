{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Dataset and DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idea è quella di dividere il dataset in batch. Questo può essere fatto attraverso pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = torch.from_numpy(xy[:,[0]]) # n_samples,1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples,n_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 1/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 21/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 41/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 1/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 21/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 41/45, inputs torch.Size([4, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs,labels) in enumerate(dataloader):\n",
    "        if i%20 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Transform on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self,transform=None):\n",
    "        xy = np.loadtxt('wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "        self.x = xy[:,1:]\n",
    "        self.y = xy[:,[0]] # n_samples,1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index],self.y[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self,sample):\n",
    "        inputs, labels = sample\n",
    "        return torch.from_numpy(inputs),torch.from_numpy(labels)\n",
    "    \n",
    "class MulTransform:\n",
    "    def __init__(self,factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self,sample):\n",
    "        inputs, labels = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset(transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
      "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
      "        2.1300e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "composed = torchvision.transforms.Compose([ToTensor(),MulTransform(2)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features,labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Softmax and Cross Entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. basic information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(y_i) = \\frac{\\exp^{y_i}}{\\sum_i \\exp^{y_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "x = np.array([2.0,1.0,0.1])\n",
    "\n",
    "outputs = softmax(x)\n",
    "\n",
    "print('softmax numpy:',outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax torch: tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0,1.0,0.1])\n",
    "\n",
    "outputs = torch.softmax(x,dim=0)\n",
    "\n",
    "print('softmax torch:',outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy\n",
    "\n",
    "$$H(p,q) = -\\frac{1}{N} \\sum_i Y_i \\log \\hat{Y_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual,predicted):\n",
    "    loss = -np.sum(actual*np.log(predicted))\n",
    "    return loss\n",
    "\n",
    "Y = np.array([1,0,0])\n",
    "\n",
    "Y_pred_good = np.array([0.7,0.2,0.1])\n",
    "\n",
    "Y_pred_bad = np.array([0.1,0.3,0.6])\n",
    "\n",
    "l1 = cross_entropy(Y,Y_pred_good)\n",
    "\n",
    "l2 = cross_entropy(Y,Y_pred_bad)\n",
    "\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "\n",
    "print(f'Loss2 numpy: {l2:.4f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso della crossentropy nn.CrossEntropyLoss() fa già la softmax, quindi non è necessario aggiungerla.\n",
    "\n",
    "Inoltre, basta mettere la giusta classe in Y, non serve l'one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1: 0.4170\n",
      "PyTorch Loss2: 1.8406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0])\n",
    "\n",
    "# nsamples x nclasses = 1 x 3\n",
    "Y_pred_good = torch.tensor([[2.0,1.0,0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5,2.0,0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good,Y)\n",
    "l2 = loss(Y_pred_bad,Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f}')\n",
    "\n",
    "print(f'PyTorch Loss2: {l2.item():.4f}')\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "_, predictions1 = torch.max(Y_pred_good,1)\n",
    "_, predictions2 = torch.max(Y_pred_bad,1)\n",
    "\n",
    "print(predictions1,predictions2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loss in pytorch permette di ricevere in input più di un sample alla volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1: 0.1393\n",
      "PyTorch Loss2: 1.4073\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([2,0,1])\n",
    "\n",
    "# nsamples x nclasses = 1 x 3\n",
    "Y_pred_good = torch.tensor([[2.0,1.0,10.0], [2.0,1.0,0.1],[2.0,10.0,0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5,2.0,0.3],[0.5,2.0,0.3],[0.5,2.0,0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good,Y)\n",
    "l2 = loss(Y_pred_bad,Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f}')\n",
    "\n",
    "print(f'PyTorch Loss2: {l2.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NN can be used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no softmax !!! because it is included in the loss\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size=28*28,hidden_size=5,num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have binary classification, we can use sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetBinary(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size,1)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # Sigmoid for binary classification\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size=28*28,hidden_size=5,num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Activation functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax utile per ultimo livello di una multiclasse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le funzioni di attivazione possono essere aggiunte come attributi di classe o applicate direttamente nel forward\n",
    "\n",
    "Sono disponibili sia in torch. o in torch.nn e anche torch.nn.functional. A esempio, la leaky relu è disponibile solo in torch functional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. FFN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d991db2ed2e241bb9e1ec33ec82b442e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c4fe53c4fb4d0e84dfd9ad792c4ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3aeafebb0dc44cfafbab2d2795eace1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb89d28a624a4f938d2a0f03723c0abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "                        root='./data', \n",
    "                        train=True,\n",
    "                        transform=transforms.ToTensor(), \n",
    "                        download=True\n",
    ") \n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "                        root='./data', \n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor()\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                        dataset=train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                        dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(train_dataset[i][0].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.l1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no softmax cause we're using cross entropy\n",
    "        # that includes softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size,hidden_size,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "n_total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, step 100/600, loss = 0.1158\n",
      "epoch 1/5, step 200/600, loss = 0.0888\n",
      "epoch 1/5, step 300/600, loss = 0.0549\n",
      "epoch 1/5, step 400/600, loss = 0.1288\n",
      "epoch 1/5, step 500/600, loss = 0.0715\n",
      "epoch 1/5, step 600/600, loss = 0.1067\n",
      "epoch 2/5, step 100/600, loss = 0.0439\n",
      "epoch 2/5, step 200/600, loss = 0.1145\n",
      "epoch 2/5, step 300/600, loss = 0.0683\n",
      "epoch 2/5, step 400/600, loss = 0.0594\n",
      "epoch 2/5, step 500/600, loss = 0.0541\n",
      "epoch 2/5, step 600/600, loss = 0.0323\n",
      "epoch 3/5, step 100/600, loss = 0.0559\n",
      "epoch 3/5, step 200/600, loss = 0.0296\n",
      "epoch 3/5, step 300/600, loss = 0.0377\n",
      "epoch 3/5, step 400/600, loss = 0.0387\n",
      "epoch 3/5, step 500/600, loss = 0.0541\n",
      "epoch 3/5, step 600/600, loss = 0.0649\n",
      "epoch 4/5, step 100/600, loss = 0.0692\n",
      "epoch 4/5, step 200/600, loss = 0.0251\n",
      "epoch 4/5, step 300/600, loss = 0.0427\n",
      "epoch 4/5, step 400/600, loss = 0.0393\n",
      "epoch 4/5, step 500/600, loss = 0.0214\n",
      "epoch 4/5, step 600/600, loss = 0.0659\n",
      "epoch 5/5, step 100/600, loss = 0.0339\n",
      "epoch 5/5, step 200/600, loss = 0.0449\n",
      "epoch 5/5, step 300/600, loss = 0.0295\n",
      "epoch 5/5, step 400/600, loss = 0.0281\n",
      "epoch 5/5, step 500/600, loss = 0.0144\n",
      "epoch 5/5, step 600/600, loss = 0.0491\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.46\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images,labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value,index)\n",
    "        _,predictions = torch.max(outputs,1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 28, 28])\n",
      "9, real labels: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for images,labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value,index)\n",
    "        _,predictions = torch.max(outputs,1)\n",
    "        wrong = (predictions != labels)\n",
    "        miss_imgs = images[wrong]\n",
    "        if miss_imgs.shape[0] == 0:\n",
    "            continue\n",
    "        print(miss_imgs.shape)\n",
    "        miss_labels = labels[wrong]\n",
    "        print(f\"{predictions[wrong][1]}, real labels: {miss_labels[1]}\")\n",
    "        plt.imshow(miss_imgs[0].reshape(28,28),cmap='gray')\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7885e45cb346868c0bfa762d57fc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])  \n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "                        root='./data',\n",
    "                        train=True,\n",
    "                        transform=transform,\n",
    "                        download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "                        root='./data',\n",
    "                        train=False,\n",
    "                        transform=transform,\n",
    "                        download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                        dataset=train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                        dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False\n",
    ")\n",
    "\n",
    "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,kernel_size=5)\n",
    "        self.ffl = nn.Sequential(\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(16*5*5,120),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(120,84),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(84,10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ffl(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, step 1/12500, loss = 2.3613\n",
      "epoch 1/5, step 1001/12500, loss = 1.7559\n",
      "epoch 1/5, step 2001/12500, loss = 1.4809\n",
      "epoch 1/5, step 3001/12500, loss = 1.6062\n",
      "epoch 1/5, step 4001/12500, loss = 1.6271\n",
      "epoch 1/5, step 5001/12500, loss = 2.3314\n",
      "epoch 1/5, step 6001/12500, loss = 2.2061\n",
      "epoch 1/5, step 7001/12500, loss = 1.7150\n",
      "epoch 1/5, step 8001/12500, loss = 1.5751\n",
      "epoch 1/5, step 9001/12500, loss = 1.2920\n",
      "epoch 1/5, step 10001/12500, loss = 1.2950\n",
      "epoch 1/5, step 11001/12500, loss = 1.4124\n",
      "epoch 1/5, step 12001/12500, loss = 1.1792\n",
      "epoch 2/5, step 1/12500, loss = 2.0352\n",
      "epoch 2/5, step 1001/12500, loss = 1.7764\n",
      "epoch 2/5, step 2001/12500, loss = 2.5816\n",
      "epoch 2/5, step 3001/12500, loss = 0.4466\n",
      "epoch 2/5, step 4001/12500, loss = 1.9209\n",
      "epoch 2/5, step 5001/12500, loss = 1.5830\n",
      "epoch 2/5, step 6001/12500, loss = 1.5501\n",
      "epoch 2/5, step 7001/12500, loss = 1.3234\n",
      "epoch 2/5, step 8001/12500, loss = 1.2816\n",
      "epoch 2/5, step 9001/12500, loss = 1.0983\n",
      "epoch 2/5, step 10001/12500, loss = 1.2681\n",
      "epoch 2/5, step 11001/12500, loss = 1.5426\n",
      "epoch 2/5, step 12001/12500, loss = 0.4394\n",
      "epoch 3/5, step 1/12500, loss = 2.2349\n",
      "epoch 3/5, step 1001/12500, loss = 0.8601\n",
      "epoch 3/5, step 2001/12500, loss = 0.7874\n",
      "epoch 3/5, step 3001/12500, loss = 2.3983\n",
      "epoch 3/5, step 4001/12500, loss = 1.2768\n",
      "epoch 3/5, step 5001/12500, loss = 0.6749\n",
      "epoch 3/5, step 6001/12500, loss = 1.1332\n",
      "epoch 3/5, step 7001/12500, loss = 0.8596\n",
      "epoch 3/5, step 8001/12500, loss = 1.7037\n",
      "epoch 3/5, step 9001/12500, loss = 1.4945\n",
      "epoch 3/5, step 10001/12500, loss = 2.2228\n",
      "epoch 3/5, step 11001/12500, loss = 0.1099\n",
      "epoch 3/5, step 12001/12500, loss = 1.0515\n",
      "epoch 4/5, step 1/12500, loss = 1.8915\n",
      "epoch 4/5, step 1001/12500, loss = 0.9932\n",
      "epoch 4/5, step 2001/12500, loss = 1.2405\n",
      "epoch 4/5, step 3001/12500, loss = 1.2163\n",
      "epoch 4/5, step 4001/12500, loss = 1.3213\n",
      "epoch 4/5, step 5001/12500, loss = 0.8706\n",
      "epoch 4/5, step 6001/12500, loss = 1.4041\n",
      "epoch 4/5, step 7001/12500, loss = 1.0338\n",
      "epoch 4/5, step 8001/12500, loss = 0.8136\n",
      "epoch 4/5, step 9001/12500, loss = 1.2418\n",
      "epoch 4/5, step 10001/12500, loss = 1.0945\n",
      "epoch 4/5, step 11001/12500, loss = 2.6621\n",
      "epoch 4/5, step 12001/12500, loss = 2.0883\n",
      "epoch 5/5, step 1/12500, loss = 1.8540\n",
      "epoch 5/5, step 1001/12500, loss = 0.9106\n",
      "epoch 5/5, step 2001/12500, loss = 1.7098\n",
      "epoch 5/5, step 3001/12500, loss = 0.7587\n",
      "epoch 5/5, step 4001/12500, loss = 1.4549\n",
      "epoch 5/5, step 5001/12500, loss = 0.6486\n",
      "epoch 5/5, step 6001/12500, loss = 1.3542\n",
      "epoch 5/5, step 7001/12500, loss = 0.2073\n",
      "epoch 5/5, step 8001/12500, loss = 1.0295\n",
      "epoch 5/5, step 9001/12500, loss = 1.0540\n",
      "epoch 5/5, step 10001/12500, loss = 0.7214\n",
      "epoch 5/5, step 11001/12500, loss = 0.9402\n",
      "epoch 5/5, step 12001/12500, loss = 1.2345\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 59.81\n",
      "Accuracy of plane: 62.6 %\n",
      "Accuracy of car: 68.6 %\n",
      "Accuracy of bird: 48.3 %\n",
      "Accuracy of cat: 39.7 %\n",
      "Accuracy of deer: 47.8 %\n",
      "Accuracy of dog: 48.0 %\n",
      "Accuracy of frog: 76.2 %\n",
      "Accuracy of horse: 67.4 %\n",
      "Accuracy of ship: 71.4 %\n",
      "Accuracy of truck: 68.1 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images,labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value,index)\n",
    "        _,predicted = torch.max(outputs,1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if labels[i] == predicted[i]:\n",
    "                n_class_correct[labels[i]]+=1\n",
    "            n_class_samples[labels[i]] +=1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {acc}')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
