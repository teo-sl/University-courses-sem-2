{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets imagen_pytorch transformers einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from transformers import T5Tokenizer, T5EncoderModel, T5Config\n",
    "from einops import rearrange\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3128974198\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# We will be saving checkpoints to our google drive so we can download them \n",
    "# later\n",
    "#model_save_dir = \"/content/drive/MyDrive/imagen_colab/\"\n",
    "#if not os.path.exists(model_save_dir):\n",
    "#  os.mkdir(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(name, labels, max_length = 256):\n",
    "    if os.path.isfile(name):\n",
    "        return torch.load(name)\n",
    "    \n",
    "    model_name = 'google/t5-v1_1-base'\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=max_length)\n",
    "\n",
    "    model = T5EncoderModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    def photo_prefix(noun):\n",
    "        if noun[0] in ['a', 'e', 'i', 'o', 'u']:\n",
    "            return 'a photo of an ' + noun\n",
    "        return 'a photo of a ' + noun\n",
    "\n",
    "    texts = [photo_prefix(x) for x in labels]\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = 'longest',\n",
    "        max_length = max_length,\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=encoded.input_ids , attention_mask=encoded.attention_mask)\n",
    "        encoded_text = output.last_hidden_state.detach()\n",
    "\n",
    "    attn_mask = encoded.attention_mask.bool()\n",
    "    \n",
    "    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.)\n",
    "    \n",
    "    torch.save(encoded_text, name)\n",
    "    \n",
    "    return encoded_text\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, embeddings, transform=None):\n",
    "        assert len(hf_dataset.features['label'].names) == embeddings.shape[0]\n",
    "        \n",
    "        self.data = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = sample['img']\n",
    "        label = sample['label']\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        text_embedding = self.embeddings[label]\n",
    "        \n",
    "        return img, text_embedding.clone()\n",
    "    \n",
    "def make(config):\n",
    "    cfar = load_dataset('cifar10')\n",
    "    labels = cfar['train'].features['label'].names\n",
    "    text_embeddings = get_text_embeddings(\"cifar10-embeddings.pkl\", labels)\n",
    "\n",
    "    unet = Unet(\n",
    "      dim = config.dim, # the \"Z\" layer dimension, i.e. the number of filters the outputs to the first layer\n",
    "      cond_dim = config.cond_dim,\n",
    "      dim_mults = config.dim_mults, # the channel dimensions inside the model (multiplied by dim)\n",
    "      num_resnet_blocks = config.num_resnet_blocks,\n",
    "      layer_attns = (False,) + (True,) * (len(config.dim_mults) - 1),\n",
    "      layer_cross_attns = (False,) + (True,) * (len(config.dim_mults) - 1)\n",
    "    )\n",
    "\n",
    "    imagen = Imagen(\n",
    "        unets = unet,\n",
    "        image_sizes = config.image_sizes,\n",
    "        timesteps = config.timesteps,\n",
    "        cond_drop_prob = config.cond_drop_prob,\n",
    "        dynamic_thresholding = config.dynamic_thresholding,\n",
    "    ).cuda()\n",
    "\n",
    "    trainer = ImagenTrainer(imagen, lr=config.lr)\n",
    "\n",
    "    ds = HFDataset(cfar['train'], text_embeddings, transform=T.Compose([ T.RandomHorizontalFlip(), T.ToTensor() ]))\n",
    "\n",
    "\n",
    "    tst_ds = HFDataset(cfar['test'], text_embeddings, transform=T.Compose([ T.RandomHorizontalFlip(), T.ToTensor() ]))\n",
    "\n",
    "    trainer.add_train_dataset(ds, batch_size = config.batch_size)\n",
    "    trainer.add_valid_dataset(tst_ds, batch_size=config.batch_size)\n",
    "\n",
    "    return trainer, text_embeddings, labels\n",
    "\n",
    "\n",
    "def train(trainer, text_embeddings, labels, config, sample_factor=None, validate_every=None, save_every=None):\n",
    "    assert config.model_save_dir[-1] == '/'\n",
    "\n",
    "    sample_every = 10\n",
    "\n",
    "    for i in range(config.steps):\n",
    "        loss = trainer.train_step(max_batch_size = config.batch_size)\n",
    "\n",
    "        wandb.log({'train_loss': loss}, step=i)\n",
    "\n",
    "        if validate_every is not None and i % validate_every == 0:\n",
    "            avg_loss = 0\n",
    "            for _ in range(100):\n",
    "                valid_loss = trainer.valid_step(unet_number=1, max_batch_size=config.batch_size)\n",
    "                avg_loss += valid_loss\n",
    "            wandb.log({'valid loss': avg_loss}, step=i)\n",
    "\n",
    "        if sample_factor is not None and i % sample_every == 0:\n",
    "            images = trainer.sample(text_embeds=text_embeddings, batch_size = config.batch_size, return_pil_images = True)\n",
    "            samples = []\n",
    "            for j, img in enumerate(images):\n",
    "                samples.append(wandb.Image(img, caption=labels[j]))\n",
    "            wandb.log({\"samples\": samples}, step=i)\n",
    "            sample_every = int(sample_every * sample_factor)\n",
    "\n",
    "        if save_every is not None and i != 0 and i % save_every == 0:\n",
    "          trainer.save(f\"{config.model_save_dir}{wandb.run.name}-{i}.ckpt\")\n",
    "    \n",
    "    # final save at the end if we did not already save this round\n",
    "    if save_every is not None and i % save_every != 0:\n",
    "      trainer.save(f\"{config.model_save_dir}{wandb.run.name}-{i}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"steps\": 200_000,\n",
    "    \"dim\": 128,\n",
    "    \"cond_dim\": 128,\n",
    "    \"dim_mults\": (1, 2, 4),\n",
    "    \"image_sizes\": 32,\n",
    "    \"timesteps\": 250,\n",
    "    \"cond_drop_prob\": 0.1,\n",
    "    \"batch_size\": 64,\n",
    "    'lr': 1e-4,\n",
    "    'num_resnet_blocks': 3,\n",
    "    \"model_save_dir\": model_save_dir,\n",
    "    \"dynamic_thresholding\": True\n",
    "}\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "def build(hyperparams):\n",
    "    \n",
    "    with wandb.init(project='cifar10-imagen', config=hyperparams):\n",
    "        \n",
    "        config = wandb.config\n",
    "        \n",
    "        trainer, embeddings, labels = make(config)\n",
    "        \n",
    "        train(trainer, embeddings, labels, config, sample_factor=1.3, validate_every=None, save_every=10_000)\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "trainer = build(hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
